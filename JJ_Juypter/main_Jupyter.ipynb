{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auxFuncs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...ok! Python3 loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"...ok! Python3 loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'02_08_2022_21_27_23'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "sourceDir = \"/Users/jordan/ThesisMARL/SURPL-V2/JJ_Juypter/results\"\n",
    "fileOut = open(sourceDir + \"/PPO.txt\", \"w+\")\n",
    "tfLogs = sourceDir + \"/logs\"\n",
    "now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(sourceDir + \"/logs\", histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "#       SMART BUILDING\n",
    "# ------------------------------\n",
    "class SmartBuildingEnv(Env):\n",
    "    def __init__(self):\n",
    "        self.viewer = None\n",
    "        self.load = 0\n",
    "        self.demand = []\n",
    "        self.timeStep = 0\n",
    "        self.deltaUtilization = 0\n",
    "        self.penalty = 0\n",
    "        self.action_space = None\n",
    "        self.observation_space = None\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        # self.load = 0\n",
    "        # MOVE TO 24 HOURS FOR INITAL RESULTS NOW THAT PRELIM RESULTS ESTABLISHED\n",
    "        self.demand = [3, 1, 1]\n",
    "        # self.demand = np.array([3,1,1])\n",
    "        # self.demand = [3, 2, 1]\n",
    "        # self.timeStep +=1 == self.demand[self.timeStep]\n",
    "        self.timeStep = 0\n",
    "        self.deltaUtilization = abs(self.demand[self.timeStep] - self.load)\n",
    "        self.penalty = (self.deltaUtilization) ** 2\n",
    "        self.action_space = Box(low=0, high=self.demand[self.timeStep], shape=(1,), dtype=float)\n",
    "        # OBSERVATION SPACE CAN ALSO INCLUDE DEMAND CHARGE, TIME, ETC -> 1 INSUFFICIENT FOR MORE RESULTS\n",
    "        self.observation_space = Box(low=0, high=self.deltaUtilization, shape=(1,), dtype=float)\n",
    "        # return np.array(0)\n",
    "        return np.array([0])\n",
    "        # pass\n",
    "        # return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        info = {}\n",
    "        reward = 0\n",
    "        done:bool = False\n",
    "        self.load = action[0]\n",
    "\n",
    "        print(\"SB stepping: \", round(action[0], 2))\n",
    "        print(\"*\"*25)\n",
    "        print(\"*\"*25)\n",
    "        print(\"SB stepping: \", round(action[0], 2), file=fileOut)\n",
    "        print(\"*\"*25, file=fileOut)\n",
    "        print(\"*\"*25, file=fileOut)\n",
    "        \n",
    "\n",
    "        print(\"Timestep: \", self.timeStep)\n",
    "        print(\"-\"*5)\n",
    "        print(\"Load: \", round(self.load, 2))\n",
    "        print(\"Demand: \", self.demand[self.timeStep])\n",
    "        print(\"Timestep: \", self.timeStep, file=fileOut)\n",
    "        print(\"-\"*5, file=fileOut)\n",
    "        print(\"Load: \", round(self.load, 2), file=fileOut)\n",
    "        print(\"Demand: \", self.demand[self.timeStep], file=fileOut)\n",
    "\n",
    "\n",
    "        self.deltaUtilization = abs(self.demand[self.timeStep] - self.load)\n",
    "        self.penalty = (self.deltaUtilization) ** 2\n",
    "        \n",
    "        observation = self.demand[self.timeStep]\n",
    "        \n",
    "        print(\"Penalty: \", round(self.penalty, 2))\n",
    "        print(\"Delta \", round(self.deltaUtilization, 2))\n",
    "        print(\"Observation: \", observation)\n",
    "        print(\"-\"*25)\n",
    "        print(\"Penalty: \", round(self.penalty, 2), file=fileOut)\n",
    "        print(\"Delta \", round(self.deltaUtilization, 2), file=fileOut)\n",
    "        print(\"Observation: \", observation, file=fileOut)\n",
    "        print(\"-\"*25, file=fileOut)\n",
    "        \n",
    "        # STILL NEED PEAK DEMAND IN REWARD\n",
    "        # HOW DO WE CONTRIBUTE TO THIS REWARD?\n",
    "        reward = 0\n",
    "        if self.load != 0:\n",
    "            reward -= self.load + self.penalty\n",
    "        else:\n",
    "            reward -= self.load + 1 + self.penalty\n",
    "            \n",
    "        # REWARD CAN BE DIFFERENCE BETWEEN COSTS\n",
    "\n",
    "        self.timeStep += 1\n",
    "        done = True if self.timeStep > 2 else done\n",
    "            \n",
    "        \n",
    "        return np.array([observation]), reward, done, info\n",
    "    \n",
    "    def render(self, mode=\"human\"):\n",
    "        screen_h = 600\n",
    "        screen_w = 1000\n",
    "        if self.viewer is None:\n",
    "            self.viewer = rendering.Viewer(screen_w, screen_h, \"SB\")\n",
    "            \n",
    "        return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n",
    "    \n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SmartBuildingEnv()\n",
    "# env = DummyVecEnv([lambda: env])\n",
    "batch_size = 500\n",
    "ep_per_batch = 10 * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SB stepping:  2.96\n",
      "*************************\n",
      "*************************\n",
      "Timestep:  0\n",
      "-----\n",
      "Load:  2.96\n",
      "Demand:  3\n",
      "Penalty:  0.0\n",
      "Delta  0.04\n",
      "Observation:  3\n",
      "-------------------------\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The observation returned by `step()` method must be a numpy array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tp/2k_8shj111j97x1c_nmhwzm40000gn/T/ipykernel_27221/1477142464.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheck_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/envs/agentppo/lib/python3.7/site-packages/stable_baselines3/common/env_checker.py\u001b[0m in \u001b[0;36mcheck_env\u001b[0;34m(env, warn, skip_render_check)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;31m# ============ Check the returned values ===============\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0m_check_returned_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;31m# ==== Check the render method and the declared render modes ====\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/envs/agentppo/lib/python3.7/site-packages/stable_baselines3/common/env_checker.py\u001b[0m in \u001b[0;36m_check_returned_values\u001b[0;34m(env, observation_space, action_space)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0m_check_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# We also allow int because the reward will be cast to float\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.11/envs/agentppo/lib/python3.7/site-packages/stable_baselines3/common/env_checker.py\u001b[0m in \u001b[0;36m_check_obs\u001b[0;34m(obs, observation_space, method_name)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"The observation returned by `{method_name}()` method must be an int\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0m_is_numpy_array_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"The observation returned by `{method_name}()` method must be a numpy array\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     assert observation_space.contains(\n",
      "\u001b[0;31mAssertionError\u001b[0m: The observation returned by `step()` method must be a numpy array"
     ]
    }
   ],
   "source": [
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=tfLogs)\n",
    "model.learn(total_timesteps=ep_per_batch, reset_num_timesteps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset() \n",
    "\n",
    "for i in range(batch_size):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    # action = env.action_space.sample()\n",
    "    \n",
    "    obs, reward, done, info = env.step(action)\n",
    "    # if ep_per_batch % 2000 == 0:\n",
    "    # env.render()\n",
    "    if done:\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100, render=True)\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=1000)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"results/PPO_\" + \"/BuildingTemp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir {sourceDir + \"/logs\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileOut.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73c9605cb908a2b9cf675ad581a88b64afe960d5109319571cfd4c2034252f42"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('agentppo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
